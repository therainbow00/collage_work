Integer types:		# bytes	#bits	Name		    Range:
char			    1	    8	    byte		   -128 - +127
unsigned char		1	    8	    byte		   0 - 255
short			    2	   16	    word		   -32768 - 32767
unsigned short		2	   16	    word		   0 - 65536
int			        4      32	    double word	   +/- 2billion
unsigned int		4	   32	    double word	   0 - ~4 billion
long			    8	   64	    quad word	   +/- 9 quintillion
unsigned long		8	   64	    quad word	   0 - 18 quintillion

2^10 ~~ 10^3	thousand
2^20 ~~ 10^6	million
2^30 ~~ 10^9	billion
2^40 ~~ 10^12	trillion
2^50 ~~ 10^15	quadrillion

#include <stdint.h>
- Gives you standard integer sizes:

int*_t		Signed integer where * == the size of the integer in bits
uint*_t		unsigned integer where "

Examples:
int8_t		A signed byte
uint8_t		An unsigned byte
int16_t		A signed word (16 bits)
uint32_t	Unsigned 32 bits
int64_t		signed 64 bits

On Intel processors, integers are arranged in least significant byte order:
- As addresses increase the significance of the byte value increases

0x A0FE

A0*256 + FE


Addresses ->
FE A0

0x12 5F A8 04
Addresses increasing ->
04 A8 5F 12

Number << # of shifts to make  (shifting bits left)

1101 1110 0001 1110  << 3
1111 0000 1111 0000

101<<2
101>>2 -> 1.01


Number >> # of shifts to make (shifting bits right)

Bit testing N:
Number & (1<<N)

  Number
& Mask
 -------

100101011 & (1<<2

  100101011
& 000000100
-----------
  000000000

Bit setting:

  100101011 | (1<<2)

  100101011
| 000000100
-----------
  100101111

Bit clearing:  Number & ~(1<<N)
  100101011
& 111110111
-----------
  100100011

(1<<A) | (1<<B) | (1<<C)

  101011010
  111000100
| 111011101
-----------
  111011111

A & B & C
  101011010
  111000100
& 111011101
-----------
  101000000


000	0
001	1
010	2
011	3
100	-4
101	-3
110	-2
111	-1


 010
+100
----
 110

 2 -> -2
010   110

  1
 101
+  1
----

 110

  1
 001
+001
----
 010
